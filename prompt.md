Project Path: src

Source Tree:

```
src
├── baml_client
│   ├── stream_types.py
│   ├── type_builder.py
│   ├── config.py
│   ├── globals.py
│   ├── sync_client.py
│   ├── async_client.py
│   ├── types.py
│   ├── type_map.py
│   ├── parser.py
│   ├── runtime.py
│   ├── inlinedbaml.py
│   ├── __init__.py
│   └── tracing.py
├── baml_src
│   ├── extract_command.baml
│   ├── clients.baml
│   └── generator.baml
└── vity
    ├── prompts.py
    ├── llm.py
    ├── config.py
    ├── schema.py
    ├── __init__.py
    └── cli.py

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/stream_types.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions
from pydantic import BaseModel, ConfigDict

import baml_py

from . import types

StreamStateValueT = typing.TypeVar('StreamStateValueT')
class StreamState(BaseModel, typing.Generic[StreamStateValueT]):
    value: StreamStateValueT
    state: typing_extensions.Literal["Pending", "Incomplete", "Complete"]
# #########################################################################
# Generated classes (2)
# #########################################################################

class ChatResponse(BaseModel):
    query_response: typing.Optional[str] = None

class Command(BaseModel):
    command: typing.Optional[str] = None

# #########################################################################
# Generated type aliases (0)
# #########################################################################

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/type_builder.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
from baml_py import type_builder
from baml_py import baml_py
from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME

class TypeBuilder(type_builder.TypeBuilder):
    def __init__(self):
        super().__init__(classes=set(
          ["ChatResponse","Command",]
        ), enums=set(
          []
        ), runtime=DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME)

    # #########################################################################
    # Generated enums 0
    # #########################################################################


    # #########################################################################
    # Generated classes 2
    # #########################################################################

    @property
    def ChatResponse(self) -> "ChatResponseViewer":
        return ChatResponseViewer(self)

    @property
    def Command(self) -> "CommandViewer":
        return CommandViewer(self)



# #########################################################################
# Generated enums 0
# #########################################################################


# #########################################################################
# Generated classes 2
# #########################################################################

class ChatResponseAst:
    def __init__(self, tb: type_builder.TypeBuilder):
        _tb = tb._tb # type: ignore (we know how to use this private attribute)
        self._bldr = _tb.class_("ChatResponse")
        self._properties: typing.Set[str] = set([  "query_response",  ])
        self._props = ChatResponseProperties(self._bldr, self._properties)

    def type(self) -> baml_py.FieldType:
        return self._bldr.field()

    @property
    def props(self) -> "ChatResponseProperties":
        return self._props


class ChatResponseViewer(ChatResponseAst):
    def __init__(self, tb: type_builder.TypeBuilder):
        super().__init__(tb)

    
    def list_properties(self) -> typing.List[typing.Tuple[str, type_builder.ClassPropertyViewer]]:
        return [(name, type_builder.ClassPropertyViewer(self._bldr.property(name))) for name in self._properties]
    


class ChatResponseProperties:
    def __init__(self, bldr: baml_py.ClassBuilder, properties: typing.Set[str]):
        self.__bldr = bldr
        self.__properties = properties # type: ignore (we know how to use this private attribute) # noqa: F821

    
    
    @property
    def query_response(self) -> type_builder.ClassPropertyViewer:
        return type_builder.ClassPropertyViewer(self.__bldr.property("query_response"))
    
    


class CommandAst:
    def __init__(self, tb: type_builder.TypeBuilder):
        _tb = tb._tb # type: ignore (we know how to use this private attribute)
        self._bldr = _tb.class_("Command")
        self._properties: typing.Set[str] = set([  "command",  ])
        self._props = CommandProperties(self._bldr, self._properties)

    def type(self) -> baml_py.FieldType:
        return self._bldr.field()

    @property
    def props(self) -> "CommandProperties":
        return self._props


class CommandViewer(CommandAst):
    def __init__(self, tb: type_builder.TypeBuilder):
        super().__init__(tb)

    
    def list_properties(self) -> typing.List[typing.Tuple[str, type_builder.ClassPropertyViewer]]:
        return [(name, type_builder.ClassPropertyViewer(self._bldr.property(name))) for name in self._properties]
    


class CommandProperties:
    def __init__(self, bldr: baml_py.ClassBuilder, properties: typing.Set[str]):
        self.__bldr = bldr
        self.__properties = properties # type: ignore (we know how to use this private attribute) # noqa: F821

    
    
    @property
    def command(self) -> type_builder.ClassPropertyViewer:
        return type_builder.ClassPropertyViewer(self.__bldr.property("command"))
    
    


```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/config.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

from __future__ import annotations

import os
import warnings
import typing_extensions
import typing
import functools

from baml_py.logging import (
    get_log_level as baml_get_log_level,
    set_log_level as baml_set_log_level,
)
from .globals import reset_baml_env_vars

rT = typing_extensions.TypeVar("rT")  # return type
pT = typing_extensions.ParamSpec("pT")  # parameters type


def _deprecated(message: str):
    def decorator(func: typing.Callable[pT, rT]) -> typing.Callable[pT, rT]:
        """Use this decorator to mark functions as deprecated.
        Every time the decorated function runs, it will emit
        a "deprecation" warning."""

        @functools.wraps(func)
        def new_func(*args: pT.args, **kwargs: pT.kwargs):
            warnings.simplefilter("always", DeprecationWarning)  # turn off filter
            warnings.warn(
                "Call to a deprecated function {}.".format(func.__name__) + message,
                category=DeprecationWarning,
                stacklevel=2,
            )
            warnings.simplefilter("default", DeprecationWarning)  # reset filter
            return func(*args, **kwargs)

        return new_func

    return decorator


@_deprecated("Use os.environ['BAML_LOG'] instead")
def get_log_level():
    """
    Get the log level for the BAML Python client.
    """
    return baml_get_log_level()


@_deprecated("Use os.environ['BAML_LOG'] instead")
def set_log_level(
    level: typing_extensions.Literal["DEBUG", "INFO", "WARN", "ERROR", "OFF"] | str,
):
    """
    Set the log level for the BAML Python client
    """
    baml_set_log_level(level)
    os.environ["BAML_LOG"] = level


@_deprecated("Use os.environ['BAML_LOG_JSON_MODE'] instead")
def set_log_json_mode():
    """
    Set the log JSON mode for the BAML Python client.
    """
    os.environ["BAML_LOG_JSON_MODE"] = "true"


@_deprecated("Use os.environ['BAML_LOG_MAX_CHUNK_LENGTH'] instead")
def set_log_max_chunk_length():
    """
    Set the maximum log chunk length for the BAML Python client.
    """
    os.environ["BAML_LOG_MAX_CHUNK_LENGTH"] = "1000"


__all__ = [
    "set_log_level",
    "get_log_level",
    "set_log_json_mode",
    "reset_baml_env_vars",
    "set_log_max_chunk_length",
]

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/globals.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

from __future__ import annotations
import os
import warnings

from baml_py import BamlCtxManager, BamlRuntime
from .inlinedbaml import get_baml_files
from typing import Dict

DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME = BamlRuntime.from_files(
  "baml_src",
  get_baml_files(),
  os.environ.copy()
)
DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX = BamlCtxManager(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME)

def reset_baml_env_vars(env_vars: Dict[str, str]):
    warnings.warn(
        "reset_baml_env_vars is deprecated and should be removed. Environment variables are now lazily loaded on each function call",
        DeprecationWarning,
        stacklevel=2
    )

__all__ = []

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/sync_client.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions
import baml_py

from . import stream_types, types, type_builder
from .parser import LlmResponseParser, LlmStreamParser
from .runtime import DoNotUseDirectlyCallManager, BamlCallOptions
from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME as __runtime__

class BamlSyncClient:
    __options: DoNotUseDirectlyCallManager
    __stream_client: "BamlStreamClient"
    __http_request: "BamlHttpRequestClient"
    __http_stream_request: "BamlHttpStreamRequestClient"
    __llm_response_parser: LlmResponseParser
    __llm_stream_parser: LlmStreamParser

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options
        self.__stream_client = BamlStreamClient(options)
        self.__http_request = BamlHttpRequestClient(options)
        self.__http_stream_request = BamlHttpStreamRequestClient(options)
        self.__llm_response_parser = LlmResponseParser(options)
        self.__llm_stream_parser = LlmStreamParser(options)

    def __getstate__(self):
        # Return state needed for pickling
        return {"options": self.__options}

    def __setstate__(self, state):
        # Restore state from pickling
        self.__options = state["options"]
        self.__stream_client = BamlStreamClient(self.__options)
        self.__http_request = BamlHttpRequestClient(self.__options)
        self.__http_stream_request = BamlHttpStreamRequestClient(self.__options)
        self.__llm_response_parser = LlmResponseParser(self.__options)
        self.__llm_stream_parser = LlmStreamParser(self.__options)

    def with_options(self,
        tb: typing.Optional[type_builder.TypeBuilder] = None,
        client_registry: typing.Optional[baml_py.baml_py.ClientRegistry] = None,
        collector: typing.Optional[typing.Union[baml_py.baml_py.Collector, typing.List[baml_py.baml_py.Collector]]] = None,
        env: typing.Optional[typing.Dict[str, typing.Optional[str]]] = None,
    ) -> "BamlSyncClient":
        options: BamlCallOptions = {}
        if tb is not None:
            options["tb"] = tb
        if client_registry is not None:
            options["client_registry"] = client_registry
        if collector is not None:
            options["collector"] = collector
        if env is not None:
            options["env"] = env
        return BamlSyncClient(self.__options.merge_options(options))

    @property
    def stream(self):
      return self.__stream_client

    @property
    def request(self):
      return self.__http_request

    @property
    def stream_request(self):
      return self.__http_stream_request

    @property
    def parse(self):
      return self.__llm_response_parser

    @property
    def parse_stream(self):
      return self.__llm_stream_parser
    
    def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = self.__options.merge_options(baml_options).call_function_sync(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.ChatResponse, result.cast_to(types, types, stream_types, False, __runtime__))
    def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = self.__options.merge_options(baml_options).call_function_sync(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.ChatResponse, result.cast_to(types, types, stream_types, False, __runtime__))
    def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = self.__options.merge_options(baml_options).call_function_sync(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.Command, result.cast_to(types, types, stream_types, False, __runtime__))
    def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = self.__options.merge_options(baml_options).call_function_sync(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.Command, result.cast_to(types, types, stream_types, False, __runtime__))
    


class BamlStreamClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlSyncStream[stream_types.ChatResponse, types.ChatResponse]:
        ctx, result = self.__options.merge_options(baml_options).create_sync_stream(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlSyncStream[stream_types.ChatResponse, types.ChatResponse](
          result,
          lambda x: typing.cast(stream_types.ChatResponse, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.ChatResponse, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlSyncStream[stream_types.ChatResponse, types.ChatResponse]:
        ctx, result = self.__options.merge_options(baml_options).create_sync_stream(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlSyncStream[stream_types.ChatResponse, types.ChatResponse](
          result,
          lambda x: typing.cast(stream_types.ChatResponse, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.ChatResponse, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlSyncStream[stream_types.Command, types.Command]:
        ctx, result = self.__options.merge_options(baml_options).create_sync_stream(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlSyncStream[stream_types.Command, types.Command](
          result,
          lambda x: typing.cast(stream_types.Command, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.Command, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlSyncStream[stream_types.Command, types.Command]:
        ctx, result = self.__options.merge_options(baml_options).create_sync_stream(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlSyncStream[stream_types.Command, types.Command](
          result,
          lambda x: typing.cast(stream_types.Command, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.Command, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    

class BamlHttpRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    

class BamlHttpStreamRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = self.__options.merge_options(baml_options).create_http_request_sync(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    

b = BamlSyncClient(DoNotUseDirectlyCallManager({}))
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/async_client.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions
import baml_py

from . import stream_types, types, type_builder
from .parser import LlmResponseParser, LlmStreamParser
from .runtime import DoNotUseDirectlyCallManager, BamlCallOptions
from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME as __runtime__


class BamlAsyncClient:
    __options: DoNotUseDirectlyCallManager
    __stream_client: "BamlStreamClient"
    __http_request: "BamlHttpRequestClient"
    __http_stream_request: "BamlHttpStreamRequestClient"
    __llm_response_parser: LlmResponseParser
    __llm_stream_parser: LlmStreamParser

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options
        self.__stream_client = BamlStreamClient(options)
        self.__http_request = BamlHttpRequestClient(options)
        self.__http_stream_request = BamlHttpStreamRequestClient(options)
        self.__llm_response_parser = LlmResponseParser(options)
        self.__llm_stream_parser = LlmStreamParser(options)

    def with_options(self,
        tb: typing.Optional[type_builder.TypeBuilder] = None,
        client_registry: typing.Optional[baml_py.baml_py.ClientRegistry] = None,
        collector: typing.Optional[typing.Union[baml_py.baml_py.Collector, typing.List[baml_py.baml_py.Collector]]] = None,
        env: typing.Optional[typing.Dict[str, typing.Optional[str]]] = None,
    ) -> "BamlAsyncClient":
        options: BamlCallOptions = {}
        if tb is not None:
            options["tb"] = tb
        if client_registry is not None:
            options["client_registry"] = client_registry
        if collector is not None:
            options["collector"] = collector
        if env is not None:
            options["env"] = env
        return BamlAsyncClient(self.__options.merge_options(options))

    @property
    def stream(self):
      return self.__stream_client

    @property
    def request(self):
      return self.__http_request

    @property
    def stream_request(self):
      return self.__http_stream_request

    @property
    def parse(self):
      return self.__llm_response_parser

    @property
    def parse_stream(self):
      return self.__llm_stream_parser
    
    async def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = await self.__options.merge_options(baml_options).call_function_async(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.ChatResponse, result.cast_to(types, types, stream_types, False, __runtime__))
    async def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = await self.__options.merge_options(baml_options).call_function_async(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.ChatResponse, result.cast_to(types, types, stream_types, False, __runtime__))
    async def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = await self.__options.merge_options(baml_options).call_function_async(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.Command, result.cast_to(types, types, stream_types, False, __runtime__))
    async def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = await self.__options.merge_options(baml_options).call_function_async(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return typing.cast(types.Command, result.cast_to(types, types, stream_types, False, __runtime__))
    


class BamlStreamClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse](
          result,
          lambda x: typing.cast(stream_types.ChatResponse, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.ChatResponse, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlStream[stream_types.ChatResponse, types.ChatResponse](
          result,
          lambda x: typing.cast(stream_types.ChatResponse, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.ChatResponse, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.Command, types.Command]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlStream[stream_types.Command, types.Command](
          result,
          lambda x: typing.cast(stream_types.Command, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.Command, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.BamlStream[stream_types.Command, types.Command]:
        ctx, result = self.__options.merge_options(baml_options).create_async_stream(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        })
        return baml_py.BamlStream[stream_types.Command, types.Command](
          result,
          lambda x: typing.cast(stream_types.Command, x.cast_to(types, types, stream_types, True, __runtime__)),
          lambda x: typing.cast(types.Command, x.cast_to(types, types, stream_types, False, __runtime__)),
          ctx,
        )
    

class BamlHttpRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    async def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    async def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    async def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    async def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="request")
        return result
    

class BamlHttpStreamRequestClient:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    async def GenerateChatResponseGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateChatResponseGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    async def GenerateChatResponseOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateChatResponseOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    async def GenerateCommandGemeni(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateCommandGemeni", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    async def GenerateCommandOpenAI(self, terminal_history: str,user_input: str,
        baml_options: BamlCallOptions = {},
    ) -> baml_py.baml_py.HTTPRequest:
        result = await self.__options.merge_options(baml_options).create_http_request_async(function_name="GenerateCommandOpenAI", args={
            "terminal_history": terminal_history,"user_input": user_input,
        }, mode="stream")
        return result
    

b = BamlAsyncClient(DoNotUseDirectlyCallManager({}))
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/types.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions
from enum import Enum


from pydantic import BaseModel, ConfigDict


import baml_py

CheckT = typing_extensions.TypeVar('CheckT')
CheckName = typing_extensions.TypeVar('CheckName', bound=str)

class Check(BaseModel):
    name: str
    expression: str
    status: str
class Checked(BaseModel, typing.Generic[CheckT, CheckName]):
    value: CheckT
    checks: typing.Dict[CheckName, Check]

def get_checks(checks: typing.Dict[CheckName, Check]) -> typing.List[Check]:
    return list(checks.values())

def all_succeeded(checks: typing.Dict[CheckName, Check]) -> bool:
    return all(check.status == "succeeded" for check in get_checks(checks))
# #########################################################################
# Generated enums (0)
# #########################################################################

# #########################################################################
# Generated classes (2)
# #########################################################################

class ChatResponse(BaseModel):
    query_response: str

class Command(BaseModel):
    command: str

# #########################################################################
# Generated type aliases (0)
# #########################################################################

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/type_map.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

from . import types
from . import stream_types


type_map = {

    "types.ChatResponse": types.ChatResponse,
    "stream_types.ChatResponse": stream_types.ChatResponse,

    "types.Command": types.Command,
    "stream_types.Command": stream_types.Command,


}
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/parser.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions

from . import stream_types, types
from .runtime import DoNotUseDirectlyCallManager, BamlCallOptions

class LlmResponseParser:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateChatResponseGemeni", llm_response=llm_response, mode="request")
        return typing.cast(types.ChatResponse, result)

    def GenerateChatResponseOpenAI(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.ChatResponse:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateChatResponseOpenAI", llm_response=llm_response, mode="request")
        return typing.cast(types.ChatResponse, result)

    def GenerateCommandGemeni(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateCommandGemeni", llm_response=llm_response, mode="request")
        return typing.cast(types.Command, result)

    def GenerateCommandOpenAI(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.Command:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateCommandOpenAI", llm_response=llm_response, mode="request")
        return typing.cast(types.Command, result)

    

class LlmStreamParser:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def GenerateChatResponseGemeni(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.ChatResponse:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateChatResponseGemeni", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.ChatResponse, result)

    def GenerateChatResponseOpenAI(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.ChatResponse:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateChatResponseOpenAI", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.ChatResponse, result)

    def GenerateCommandGemeni(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.Command:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateCommandGemeni", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.Command, result)

    def GenerateCommandOpenAI(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.Command:
        result = self.__options.merge_options(baml_options).parse_response(function_name="GenerateCommandOpenAI", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.Command, result)

    
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/runtime.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import os
import typing
import typing_extensions

import baml_py

from . import types, stream_types, type_builder
from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_RUNTIME as __runtime__, DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX as __ctx__manager__


class BamlCallOptions(typing.TypedDict, total=False):
    tb: typing_extensions.NotRequired[type_builder.TypeBuilder]
    client_registry: typing_extensions.NotRequired[baml_py.baml_py.ClientRegistry]
    env: typing_extensions.NotRequired[typing.Dict[str, typing.Optional[str]]]
    collector: typing_extensions.NotRequired[
        typing.Union[baml_py.baml_py.Collector, typing.List[baml_py.baml_py.Collector]]
    ]


class _ResolvedBamlOptions:
    tb: typing.Optional[baml_py.baml_py.TypeBuilder]
    client_registry: typing.Optional[baml_py.baml_py.ClientRegistry]
    collectors: typing.List[baml_py.baml_py.Collector]
    env_vars: typing.Dict[str, str]

    def __init__(
        self,
        tb: typing.Optional[baml_py.baml_py.TypeBuilder],
        client_registry: typing.Optional[baml_py.baml_py.ClientRegistry],
        collectors: typing.List[baml_py.baml_py.Collector],
        env_vars: typing.Dict[str, str],
    ):
        self.tb = tb
        self.client_registry = client_registry
        self.collectors = collectors
        self.env_vars = env_vars





class DoNotUseDirectlyCallManager:
    def __init__(self, baml_options: BamlCallOptions):
        self.__baml_options = baml_options

    def __getstate__(self):
        # Return state needed for pickling
        return {"baml_options": self.__baml_options}

    def __setstate__(self, state):
        # Restore state from pickling
        self.__baml_options = state["baml_options"]

    def __resolve(self) -> _ResolvedBamlOptions:
        tb = self.__baml_options.get("tb")
        if tb is not None:
            baml_tb = tb._tb  # type: ignore (we know how to use this private attribute)
        else:
            baml_tb = None
        client_registry = self.__baml_options.get("client_registry")
        collector = self.__baml_options.get("collector")
        collectors_as_list = (
            collector
            if isinstance(collector, list)
            else [collector] if collector is not None else []
        )
        env_vars = os.environ.copy()
        for k, v in self.__baml_options.get("env", {}).items():
            if v is not None:
                env_vars[k] = v
            else:
                env_vars.pop(k, None)

        return _ResolvedBamlOptions(
            baml_tb,
            client_registry,
            collectors_as_list,
            env_vars,
        )

    def merge_options(self, options: BamlCallOptions) -> "DoNotUseDirectlyCallManager":
        return DoNotUseDirectlyCallManager({**self.__baml_options, **options})

    async def call_function_async(
        self, *, function_name: str, args: typing.Dict[str, typing.Any]
    ) -> baml_py.baml_py.FunctionResult:
        resolved_options = self.__resolve()
        return await __runtime__.call_function(
            function_name,
            args,
            # ctx
            __ctx__manager__.clone_context(),
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # collectors
            resolved_options.collectors,
            # env_vars
            resolved_options.env_vars,
        )

    def call_function_sync(
        self, *, function_name: str, args: typing.Dict[str, typing.Any]
    ) -> baml_py.baml_py.FunctionResult:
        resolved_options = self.__resolve()
        ctx = __ctx__manager__.get()
        return __runtime__.call_function_sync(
            function_name,
            args,
            # ctx
            ctx,
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # collectors
            resolved_options.collectors,
            # env_vars
            resolved_options.env_vars,
        )

    def create_async_stream(
        self,
        *,
        function_name: str,
        args: typing.Dict[str, typing.Any],
    ) -> typing.Tuple[baml_py.baml_py.RuntimeContextManager, baml_py.baml_py.FunctionResultStream]:
        resolved_options = self.__resolve()
        ctx = __ctx__manager__.clone_context()
        result = __runtime__.stream_function(
            function_name,
            args,
            # this is always None, we set this later!
            # on_event
            None,
            # ctx
            ctx,
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # collectors
            resolved_options.collectors,
            # env_vars
            resolved_options.env_vars,
        )
        return ctx, result

    def create_sync_stream(
        self,
        *,
        function_name: str,
        args: typing.Dict[str, typing.Any],
    ) -> typing.Tuple[baml_py.baml_py.RuntimeContextManager, baml_py.baml_py.SyncFunctionResultStream]:
        resolved_options = self.__resolve()
        ctx = __ctx__manager__.get()
        result = __runtime__.stream_function_sync(
            function_name,
            args,
            # this is always None, we set this later!
            # on_event
            None,   
            # ctx
            ctx,
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # collectors
            resolved_options.collectors,
            # env_vars
            resolved_options.env_vars,
        )
        return ctx, result

    async def create_http_request_async(
        self,
        *,
        function_name: str,
        args: typing.Dict[str, typing.Any],
        mode: typing_extensions.Literal["stream", "request"],
    ) -> baml_py.baml_py.HTTPRequest:
        resolved_options = self.__resolve()
        return await __runtime__.build_request(
            function_name,
            args,
            # ctx
            __ctx__manager__.clone_context(),
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # env_vars
            resolved_options.env_vars,
            # is_stream
            mode == "stream",
        )

    def create_http_request_sync(
        self,
        *,
        function_name: str,
        args: typing.Dict[str, typing.Any],
        mode: typing_extensions.Literal["stream", "request"],
    ) -> baml_py.baml_py.HTTPRequest:
        resolved_options = self.__resolve()
        return __runtime__.build_request_sync(
            function_name,
            args,
            # ctx
            __ctx__manager__.get(),
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # env_vars
            resolved_options.env_vars,
            # is_stream
            mode == "stream",
        )

    def parse_response(self, *, function_name: str, llm_response: str, mode: typing_extensions.Literal["stream", "request"]) -> typing.Any:
        resolved_options = self.__resolve()
        return __runtime__.parse_llm_response(
            function_name,
            llm_response,
            # enum_module
            types,
            # cls_module
            types,
            # partial_cls_module
            stream_types,
            # allow_partials
            mode == "stream",
            # ctx
            __ctx__manager__.get(),
            # tb
            resolved_options.tb,
            # cr
            resolved_options.client_registry,
            # env_vars
            resolved_options.env_vars,
        )
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/inlinedbaml.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "client<llm> OpenAIClient {\n  provider openai-generic\n  options {\n    api_key env.BAML_API_KEY\n    base_url env.BAML_BASE_URL\n    model env.BAML_MODEL\n  }\n}\nclient<llm> GeminiClient{\n  provider google-ai\n  options {\n    api_key env.BAML_API_KEY\n    model env.BAML_MODEL\n  }\n}",
    "extract_command.baml": "class Command{\n    command string\n}\n\nclass ChatResponse{\n  query_response string\n}\n\nfunction GenerateChatResponseOpenAI(terminal_history: string, user_input: string) -> ChatResponse{\n  client OpenAIClient\n  prompt #\"\n    You are a helpful linux terminal assistant.\n    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.\n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\n\n\nfunction GenerateCommandOpenAI(terminal_history: string, user_input: string) -> Command{\n  // see ollama-clients.baml\n  client OpenAIClient\n\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. \n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\nfunction GenerateChatResponseGemeni(terminal_history: string, user_input: string) -> ChatResponse{\n  client GeminiClient\n  prompt #\"\n    You are a helpful linux terminal assistant.\n    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.\n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\n\n\nfunction GenerateCommandGemeni(terminal_history: string, user_input: string) -> Command{\n  // see ollama-clients.baml\n  client GeminiClient\n\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. \n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\ntest TestName {\n  functions [GenerateCommandOpenAI]\n  args {\n    terminal_history #\"\n     \n    \"#\n    user_input #\"\n      say hello\n    \"#\n  }\n}\n",
    "generator.baml": "generator target {\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"sync\"\n\n    // Version of runtime to generate code for (should match installed baml-py version)\n    version \"0.76.2\"\n}\n",
}

def get_baml_files():
    return _file_map
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/__init__.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

__version__ = "0.201.0"

try:
  from baml_py.safe_import import EnsureBamlPyImport
except ImportError:
  raise ImportError(f"""Update to baml-py required.
Version of baml_client generator (see generators.baml): {__version__}

Please upgrade baml-py to version "{__version__}".

$ pip install baml-py=={__version__}
$ uv add baml-py=={__version__}

If nothing else works, please ask for help:

https://github.com/boundaryml/baml/issues
https://boundaryml.com/discord
""") from None


with EnsureBamlPyImport(__version__) as e:
  e.raise_if_incompatible_version(__version__)

  from . import types
  from . import tracing
  from . import stream_types
  from . import config
  from .config import reset_baml_env_vars
  
  from .sync_client import b
  


# FOR LEGACY COMPATIBILITY, expose "partial_types" as an alias for "stream_types"
# WE RECOMMEND USERS TO USE "stream_types" INSTEAD
partial_types = stream_types

__all__ = [
  "b",
  "stream_types",
  "partial_types",
  "tracing",
  "types",
  "reset_baml_env_vars",
  "config",
]
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_client/tracing.py`:

```py
# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

from .globals import DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX

trace = DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.trace_fn
set_tags = DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.upsert_tags
def flush():
  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush()
on_log_event = DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.on_log_event


__all__ = ['trace', 'set_tags', "flush", "on_log_event"]

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_src/extract_command.baml`:

```baml
class Command{
    command string
}

class ChatResponse{
  query_response string
}

function GenerateChatResponseOpenAI(terminal_history: string, user_input: string) -> ChatResponse{
  client OpenAIClient
  prompt #"
    You are a helpful linux terminal assistant.
    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.

    Terminal History:
    ---
    {{ terminal_history }}
    ---

    User Input:
    ---
    {{ user_input }}
    ---

    {# special macro to print the output instructions. #}
    {{ ctx.output_format }}

    JSON:
  "#
}




function GenerateCommandOpenAI(terminal_history: string, user_input: string) -> Command{
  // see ollama-clients.baml
  client OpenAIClient


  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!
  prompt #"
    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. 

    Terminal History:
    ---
    {{ terminal_history }}
    ---

    User Input:
    ---
    {{ user_input }}
    ---

    {# special macro to print the output instructions. #}
    {{ ctx.output_format }}

    JSON:
  "#
}


function GenerateChatResponseGemeni(terminal_history: string, user_input: string) -> ChatResponse{
  client GeminiClient
  prompt #"
    You are a helpful linux terminal assistant.
    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.

    Terminal History:
    ---
    {{ terminal_history }}
    ---

    User Input:
    ---
    {{ user_input }}
    ---

    {# special macro to print the output instructions. #}
    {{ ctx.output_format }}

    JSON:
  "#
}




function GenerateCommandGemeni(terminal_history: string, user_input: string) -> Command{
  // see ollama-clients.baml
  client GeminiClient


  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!
  prompt #"
    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. 

    Terminal History:
    ---
    {{ terminal_history }}
    ---

    User Input:
    ---
    {{ user_input }}
    ---

    {# special macro to print the output instructions. #}
    {{ ctx.output_format }}

    JSON:
  "#
}

test TestName {
  functions [GenerateCommandOpenAI]
  args {
    terminal_history #"
     
    "#
    user_input #"
      say hello
    "#
  }
}

```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_src/clients.baml`:

```baml
client<llm> OpenAIClient {
  provider openai-generic
  options {
    api_key env.BAML_API_KEY
    base_url env.BAML_BASE_URL
    model env.BAML_MODEL
  }
}
client<llm> GeminiClient{
  provider google-ai
  options {
    api_key env.BAML_API_KEY
    model env.BAML_MODEL
  }
}
```

`/home/kalish/Documents/projects/terminal_assistant/src/baml_src/generator.baml`:

```baml
generator target {
    output_type "python/pydantic"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"

    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"

    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.76.2"
}

```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/prompts.py`:

```py
COMMAND_SYSTEM_PROMPT = """
You are a helpful linux terminal assistant.
You examine any provided terminal history and the user's request, then output exactly one shell command. You can include a comment to explain the command if absolutely necessary.
"""

CHAT_SYSTEM_PROMPT = """
    You are a helpful linux terminal assistant.
    You examine any provided terminal history and the user's request, then output a response to the user's request.
"""
```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/llm.py`:

```py
from openai import OpenAI
from baml_client.sync_client import b
from baml_client.types import Command as BamlCommand
from .config import config
from .schema import Command
from . import prompts
from typing import Optional
import os
import re



def load_configs():
    """Get OpenAI client with proper error handling"""
    api_key = None
    base_url = None
    llm_model = None
    
    # Try to get API key from config first
    if config and hasattr(config, 'vity_llm_api_key'):
        api_key = config.vity_llm_api_key
    if config and hasattr(config, 'vity_llm_base_url'):
        base_url = config.vity_llm_base_url
    if config and hasattr(config, 'vity_llm_model'):
        llm_model = config.vity_llm_model
    
    
    if not api_key:
        raise ValueError("VITY_LLM_API_KEY not found. Please run 'vity config' to set it up. Set VITY_LLM_API_KEY 'NONE' if you don't need an API key. ")
    if not base_url:
        raise ValueError("VITY_LLM_BASE_URL not found. Please run 'vity config' to set it up.")
    if not llm_model:
        raise ValueError("VITY_LLM_MODEL not found. Please run 'vity config' to set it up.")
    
    os.environ['BAML_BASE_URL'] = config.vity_llm_base_url
    os.environ['BAML_MODEL'] = config.vity_llm_model
    os.environ['BAML_LOG'] = 'error'
    if config.vity_llm_api_key and config.vity_llm_api_key != 'NONE':
        os.environ['BAML_API_KEY'] = config.vity_llm_api_key
    else:
        os.environ['BAML_API_KEY'] = ""

    

    


def remove_terminal_history_tags(text: str) -> str:
    """
    Removes anything included inside <terminal_history>...</terminal_history> tags in the given text,
    including the tags themselves.
    """
    return re.sub(r"<terminal_history>.*?</terminal_history>", "", text, flags=re.DOTALL)




def generate_command(terminal_history: Optional[str], chat_history: Optional[list], user_input: str, provider: str = "openai") -> list:
    load_configs()   
    messages = []
    if chat_history:
        messages.extend(chat_history)
    messages.append(
        {
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": f"{user_input}"
                }
            ]
        }
    )
    

    if provider == "google":
        response = b.GenerateCommandGemeni(terminal_history, user_input)
    elif provider == "openai":
        response = b.GenerateCommandOpenAI(terminal_history, user_input)
    else:
        raise ValueError(f"Unknown provider was passed: {provider} ")

    messages.append(
        {
            "role": "assistant",
            "content": [
                {
                    "type": "output_text",
                    "text": f"{response.command}"
                }
            ]
        }
    )
    return messages

def generate_chat_response(terminal_history: Optional[str], chat_history: Optional[list], user_input: str, provider: str = "openai") -> list:
    load_configs()   
    messages = []
    if chat_history:
        messages.extend(chat_history)
    messages.append(
        {
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": f"{user_input}"
                }
            ]
        }
    )
    
    if provider == "google":
        response = b.GenerateChatResponseGemeni(terminal_history, user_input)
    elif provider == "openai":
        response = b.GenerateChatResponseOpenAI(terminal_history, user_input)
    else:
        raise ValueError(f"Unknown provider was passed: {provider} ")

    messages.append(
        {
            "role": "assistant",
            "content": [
                {
                    "type": "output_text",
                    "text": f"{response.query_response}"
                }
            ]
        }
    )
    return messages

```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/config.py`:

```py
from pydantic_settings import BaseSettings
from pathlib import Path

class Config(BaseSettings):
    vity_llm_api_key: str
    vity_llm_base_url: str
    vity_llm_model: str
    class Config:
        env_file = [
            f for f in [
            ".env",
            str(Path.home() / ".config" / "vity" / ".env"),
        ] if Path(f).exists()]
        env_file_encoding = "utf-8"
        
        

# Try to load config, but don't fail if not found
try:
    config = Config()
except Exception as e:
    print(e)
    config = None
```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/schema.py`:

```py
from pydantic import BaseModel
from typing import Optional

class Command(BaseModel):
    command: str
    comment: Optional[str] = None


```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/__init__.py`:

```py
"""Vity - AI-powered terminal assistant"""

import importlib.metadata

try:
    __version__ = importlib.metadata.version("vity")
except importlib.metadata.PackageNotFoundError:
    # Fallback for development installations
    __version__ = "0.0.0+dev"

__author__ = "Kaleab Ayenew"
__email__ = "ai@enhance.care" 
```

`/home/kalish/Documents/projects/terminal_assistant/src/vity/cli.py`:

```py
#!/usr/bin/env python3
"""
Vity CLI - AI-powered terminal assistant
"""
import sys
import os
import argparse
import json
from pathlib import Path

from .llm import generate_command, generate_chat_response, remove_terminal_history_tags
from . import __version__


def check_config() -> bool:
    """Check if configuration exists"""
    config_dir = Path.home() / ".config" / "vity"
    config_file = config_dir / ".env"
    return config_file.exists()

def setup_config() -> bool:
    """Setup configuration on first run"""
    config_dir = Path.home() / ".config" / "vity"
    config_file = config_dir / ".env"
    
    if not config_file.exists():
        print("🤖 Welcome to Vity! Let's set up your OpenAI API key.")
        print("You can get one at: https://platform.openai.com/api-keys")
        print()
        
        base_url = input("Enter your LLM provider base url: ").strip()
        api_key = input("Enter your LLM provider API key[Use 'NONE' if not needed]: ").strip()
        llm_model = input("Enter LLM model name to use: ").strip()

        if not api_key:
            print("❌ API key is required")
            return False
        if not base_url:
            print("❌ Base URL is required")
            return False
        if not llm_model:
            print("❌ LLM model name is required")
        
        config_dir.mkdir(parents=True, exist_ok=True)
        config_text = f"VITY_LLM_API_KEY={api_key}\nVITY_LLM_MODEL={llm_model}\nVITY_LLM_BASE_URL={base_url}\n"
        config_file.write_text(config_text)
        
        print("✅ Configuration saved!")
        print(f"Config file: {config_file}")
        print()
        
    else:
        print("✅ Configuration already exists")
        print(f"Config file: {config_file}")
        print("Run 'vity config --reset' to reset the configuration")




def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Vity - AI-powered terminal assistant",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  vity do "find all python files"
  vity chat "explain this error"
  vity -f session.log do "fix the deployment issue"
  vity -c chat.json chat "continue our conversation"
  vity -f session.log -c chat.json do "help with this error"
  vity config --reset
  vity reinstall
  
For shell integration, run: vity install
        """
    )
    
    parser.add_argument(
        "-f", "--file", dest="history_file",
        help="Path to terminal session log file for context"
    )
    parser.add_argument(
        "-c", "--chat", dest="chat_file",
        help="Path to chat history file for conversation context"
    )
    parser.add_argument(
        "-m", "--mode", dest="interaction_mode",
        choices=["do", "chat"],
        default="do",
        help="Interaction mode (default: do)"
    )
    parser.add_argument(
        "--version", action="version", version=f"%(prog)s {__version__}"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Do command
    do_parser = subparsers.add_parser("do", help="Generate shell command")
    do_parser.add_argument("prompt", nargs="+", help="What you want to do")
    
    # Chat command
    chat_parser = subparsers.add_parser("chat", help="Chat with AI")
    chat_parser.add_argument("prompt", nargs="+", help="Your question")
    
    # Install command
    install_parser = subparsers.add_parser("install", help="Install shell integration")
    
    # Reinstall command
    reinstall_parser = subparsers.add_parser("reinstall", help="Reinstall shell integration")
    
    # Config command
    config_parser = subparsers.add_parser("config", help="Manage configuration")
    config_parser.add_argument("--reset", action="store_true", help="Reset configuration")
    config_parser.add_argument("--show", action="store_true", help="Show configuration")
    
    args = parser.parse_args()
    
    # Handle special commands first (always available)
    if args.command == "install":
        install_shell_integration()
        return
    
    if args.command == "reinstall":
        reinstall_shell_integration()
        return
    
    if args.command == "config":
        if args.reset:
            reset_config()
            return
        elif args.show:
            show_config()
            return
        else:
            setup_config()
            return
    
    # Setup config if needed (for other commands)
    if not check_config():
        setup_config()
    
    # Handle main commands
    if not args.command:
        parser.print_help()
        return
    
    if args.command in ["do", "chat"]:
        from vity.config import config
        if "googleapis" in config.vity_llm_base_url:
            provider = "google"
        else:
            provider = "openai"
        user_input = " ".join(args.prompt)
        
        # Load terminal history if provided
        terminal_history = ""
        if args.history_file:
            try:
                with open(args.history_file, "r") as f:
                    terminal_history = f.read()
            except FileNotFoundError:
                print(f"⚠️  Warning: history file '{args.history_file}' not found")
        
        # Load chat history if provided
        chat_history = []
        if args.chat_file:
            try:
                with open(args.chat_file, "r") as f:
                    chat_history = json.load(f)
            except FileNotFoundError:
                # Create empty chat history file if it doesn't exist
                chat_history = []
            except json.JSONDecodeError:
                print(f"⚠️  Warning: chat file '{args.chat_file}' contains invalid JSON, starting fresh")
                chat_history = []
        
        print("🤖 Vity is thinking...")
        
        try:
            if args.command == "do":
                
                updated_chat_history = generate_command(terminal_history, chat_history, user_input, provider)

                for message in reversed(updated_chat_history):
                    if message["role"] == "user":
                        message["content"][0]["text"] = remove_terminal_history_tags(message["content"][0]["text"]) 
                        break

                # Extract the command from the last assistant message
                last_assistant_msg = None
                for msg in reversed(updated_chat_history):
                    if msg.get("role") == "assistant":
                        last_assistant_msg = msg
                        break
                
                if last_assistant_msg:
                    # Extract command from assistant response
                    content = last_assistant_msg.get("content", [{}])[0].get("text", "")
                    if " # " in content:
                        cmd_part = content.split(" # ")[0]
                        comment_part = content.split(" # ")[1].replace(" * vity generated command", "")
                        cmd_string = f"{cmd_part} # {comment_part}"
                    else:
                        cmd_string = content
                    print(f"Command: {cmd_string}")
                    
                    # Add to bash history
                    history_file = Path.home() / ".bash_history"
                    if history_file.exists():
                        with open(history_file, "a") as f:
                            f.write(f"{cmd_string} # Vity generated\n")

                
                # Save updated chat history
                if args.chat_file:
                    with open(args.chat_file, "w") as f:
                        json.dump(updated_chat_history, f, indent=2)
                
            elif args.command == "chat":
                updated_chat_history = generate_chat_response(terminal_history, chat_history, user_input, provider)

                for message in reversed(updated_chat_history):
                    if message["role"] == "user":
                        message["content"][0]["text"] = remove_terminal_history_tags(message["content"][0]["text"]) 
                        break

                # Extract the response from the last assistant message
                last_assistant_msg = None
                for msg in reversed(updated_chat_history):
                    if msg.get("role") == "assistant":
                        last_assistant_msg = msg
                        break
                
                if last_assistant_msg:
                    content = last_assistant_msg.get("content", [{}])[0].get("text", "")
                    print(content)
                
                # Save updated chat history
                if args.chat_file:
                    with open(args.chat_file, "w") as f:
                        json.dump(updated_chat_history, f, indent=2)
                
        except Exception as e:
            print(f"❌ Error: {e}")
            sys.exit(1)


def install_shell_integration():
    """Install shell integration"""
    script_content = '''
# Vity shell integration
vity() {
    if [[ "$1" == "record" ]]; then
        shift
        log_dir="$HOME/.local/share/vity/logs"
        chat_dir="$HOME/.local/share/vity/chat"
        mkdir -p "$log_dir"
        mkdir -p "$chat_dir"
        logfile="$log_dir/$(date +%Y%m%d-%H%M%S)-$$.log"
        chatfile="$chat_dir/$(date +%Y%m%d-%H%M%S)-$$.json"
        
        export VITY_ACTIVE_LOG="$logfile"
        export VITY_ACTIVE_CHAT="$chatfile"
        echo "🔴 Starting recording session"
        echo "📝 Use 'vity do' or 'vity chat' for contextual help"
        echo "🛑 Type 'exit' to stop recording"
        
        script -f "$logfile"
        
        unset VITY_ACTIVE_LOG VITY_ACTIVE_CHAT
        echo "🟢 Recording session ended"
        
    elif [[ "$1" == "do" ]]; then
        shift
        if [[ -n "$VITY_ACTIVE_LOG" && -f "$VITY_ACTIVE_LOG" ]]; then
            command vity -f "$VITY_ACTIVE_LOG" -c "$VITY_ACTIVE_CHAT" do "$@"
        else
            echo "⚠️  No active recording. Use 'vity record' for context."
            command vity do "$@"
        fi
        
    elif [[ "$1" == "chat" ]]; then
        shift
        if [[ -n "$VITY_ACTIVE_LOG" && -f "$VITY_ACTIVE_LOG" ]]; then
            command vity -f "$VITY_ACTIVE_LOG" -c "$VITY_ACTIVE_CHAT" chat "$@"
        else
            echo "⚠️  No active recording. Use 'vity record' for context."
            command vity chat "$@"
        fi
        
    elif [[ "$1" == "status" ]]; then
        if [[ -n "$VITY_ACTIVE_LOG" ]]; then
            echo "🔴 Recording active:"
            echo "  📝 Terminal log: $VITY_ACTIVE_LOG"
            echo "  💬 Chat history: $VITY_ACTIVE_CHAT"
        else
            echo "⚫ No active recording"
        fi
        
    # Forward commands that should be handled directly by the Python CLI
    elif [[ "$1" == "install" || "$1" == "reinstall" || "$1" == "config" ]]; then
        # Call the underlying vity executable with the provided arguments unchanged
        command vity "$@"
        
    elif [[ "$1" == "help" || "$1" == "-h" || "$1" == "--help" ]]; then
        cat << 'EOF'
🤖 Vity - AI Terminal Assistant

USAGE:
    vity <command> [options] [prompt]

COMMANDS:
    do <prompt>      Generate a shell command (adds to history)
    chat <prompt>    Chat with AI about terminal/coding topics
    record           Start recording session for context
    status           Show current recording status
    config           Show configuration
    config --reset   Reset configuration (always available)
    install          Install shell integration (always available)
    reinstall        Reinstall shell integration (always available)
    help             Show this help message

EXAMPLES:
    vity do "find all python files"
    vity chat "explain this error message"
    vity record
    vity do "deploy the app"  # (with context from recording)
    vity status
    vity config --reset
    vity reinstall

CONTEXT:
    • Use 'vity record' to start capturing session context
    • Commands run during recording provide better AI responses
    • Recording captures both terminal output and chat history
    • Recording indicator (🔴) shows in your prompt
    • Use 'exit' to stop recording
EOF
        
    else
        # Show help for unknown commands or no arguments
        if [[ -n "$1" ]]; then
            echo "❌ Unknown command: $1"
            echo ""
        fi
        echo "🤖 Vity - AI Terminal Assistant"
        echo ""
        echo "Usage: vity <command> [prompt]"
        echo ""
        echo "Commands:"
        echo "  do <prompt>      Generate shell command"
        echo "  chat <prompt>    Chat with AI"
        echo "  record           Start recording session"
        echo "  status           Show recording status"
        echo "  config           Show configuration"
        echo "  config --reset   Reset configuration"
        echo "  install          Install shell integration"
        echo "  reinstall        Reinstall shell integration"
        echo "  help             Show detailed help"
        echo ""
        echo "Run 'vity help' for more details and examples."
    fi
    
    history -n 2>/dev/null || true
}
'''
    
    bashrc = Path.home() / ".bashrc"
    if bashrc.exists():
        content = bashrc.read_text()
        if "# Vity shell integration" not in content:
            with open(bashrc, "a") as f:
                f.write(f"\n{script_content}")
            print("✅ Shell integration installed!")
            print("Run 'source ~/.bashrc' or start a new terminal session")
        else:
            print("✅ Shell integration already installed")
    else:
        print("❌ ~/.bashrc not found")


def reinstall_shell_integration():
    """Reinstall shell integration (remove existing and install fresh)"""
    bashrc = Path.home() / ".bashrc"
    
    if not bashrc.exists():
        print("❌ ~/.bashrc not found")
        return
    
    print("🔄 Reinstalling shell integration...")
    
    # Read the current bashrc content
    content = bashrc.read_text()
    
    # Find and remove existing vity shell integration
    lines = content.split('\n')
    new_lines = []
    in_vity_section = False
    
    for line in lines:
        if line.strip() == "# Vity shell integration":
            in_vity_section = True
            print("🗑️  Removing existing shell integration...")
            continue
        elif in_vity_section and line.strip() == "}":
            in_vity_section = False
            continue
        elif not in_vity_section:
            new_lines.append(line)
    
    # Write the cleaned content back
    bashrc.write_text('\n'.join(new_lines))
    
    # Install fresh shell integration
    print("✨ Installing fresh shell integration...")
    install_shell_integration()



def reset_config():
    """Reset configuration"""
    config_dir = Path.home() / ".config" / "vity"
    config_file = config_dir / ".env"
    
    if config_file.exists():
        config_file.unlink()
        print("✅ Configuration reset")
    else:
        print("ℹ️  No configuration found")


def show_config():
    """Show current configuration"""
    config_dir = Path.home() / ".config" / "vity"
    config_file = config_dir / ".env"
    
    if config_file.exists():
        print(f"📁 Config file: {config_file}")
        print("🔑 API key configured")
    else:
        print("❌ No configuration found")
        print("Run 'vity config' to set up")


if __name__ == "__main__":
    main() 
```